---
description: learning
---

# Pytorch

## Notebook-Morvan course â˜¯ï¸

### â¡ torch ä¸ numpy æ— ç¼.

```python
# matrix multiplication çŸ©é˜µç‚¹ä¹˜
data = [[1,2], [3,4]]
tensor = torch.FloatTensor(data)  # è½¬æ¢æˆ32ä½æµ®ç‚¹ tensor
# correct method
print(
    '\nmatrix multiplication (matmul)',
    '\nnumpy: ', np.matmul(data, data),     # [[7, 10], [15, 22]]
    '\ntorch: ', torch.mm(tensor, tensor)   # [[7, 10], [15, 22]]
)

# !!!!  ä¸‹é¢æ˜¯é”™è¯¯çš„æ–¹æ³• !!!!
data = np.array(data)
print(
    '\nmatrix multiplication (dot)',
    '\nnumpy: ', data.dot(data),        # [[7, 10], [15, 22]] åœ¨numpy ä¸­å¯è¡Œ
    '\ntorch: ', tensor.dot(tensor)     # torch ä¼šè½¬æ¢æˆ [1,2,3,4].dot([1,2,3,4) = 30.0
)
```

### â¡ Variable

Variable æ˜¯ä¸€ä¸ªåœ°å€,é‡Œé¢ä¿å­˜çš„æ•°æ˜¯å˜åŒ–çš„.

```python
import torch
from torch.autograd import Variable # torch ä¸­ Variable æ¨¡å—

# å…ˆç”Ÿé¸¡è›‹
tensor = torch.FloatTensor([[1,2],[3,4]])
# æŠŠé¸¡è›‹æ”¾åˆ°ç¯®å­é‡Œ, requires_gradæ˜¯å‚ä¸å‚ä¸è¯¯å·®åå‘ä¼ æ’­, è¦ä¸è¦è®¡ç®—æ¢¯åº¦
variable = Variable(tensor, requires_grad=True)

print(tensor)
"""
 1  2
 3  4
[torch.FloatTensor of size 2x2]
"""

print(variable)
"""
Variable containing:
 1  2
 3  4
[torch.FloatTensor of size 2x2]
"""
```

å…³äºè®¡ç®—: **ä½† æ˜¯æ—¶åˆ»è®°ä½, Variable è®¡ç®—æ—¶, å®ƒåœ¨èƒŒæ™¯å¹•å¸ƒåé¢ä¸€æ­¥æ­¥é»˜é»˜åœ°æ­å»ºç€ä¸€ä¸ªåºå¤§çš„ç³»ç»Ÿ, å«åšè®¡ç®—å›¾, computational graph. è¿™ä¸ªå›¾æ˜¯ç”¨æ¥å¹²å˜›çš„? åŸæ¥æ˜¯å°†æ‰€æœ‰çš„è®¡ç®—æ­¥éª¤ \(èŠ‚ç‚¹\) éƒ½è¿æ¥èµ·æ¥, æœ€åè¿›è¡Œè¯¯å·®åå‘ä¼ é€’çš„æ—¶å€™, ä¸€æ¬¡æ€§å°†æ‰€æœ‰ variable é‡Œé¢çš„ä¿®æ”¹å¹…åº¦ \(æ¢¯åº¦\) éƒ½è®¡ç®—å‡ºæ¥, è€Œ tensor å°±æ²¡æœ‰è¿™ä¸ªèƒ½åŠ›å•¦.**

### â¡ Activate Function

```python
import torch
import torch.nn.functional as F     # æ¿€åŠ±å‡½æ•°éƒ½åœ¨è¿™
from torch.autograd import Variable

# åšä¸€äº›å‡æ•°æ®æ¥è§‚çœ‹å›¾åƒ
x = torch.linspace(-5, 5, 200)  # x data (tensor), shape=(100, 1)
x = Variable(x)
x_np = x.data.numpy()   # æ¢æˆ numpy array, å‡ºå›¾æ—¶ç”¨

# å‡ ç§å¸¸ç”¨çš„ æ¿€åŠ±å‡½æ•°
y_relu = F.relu(x).data.numpy()
y_sigmoid = F.sigmoid(x).data.numpy()
y_tanh = F.tanh(x).data.numpy()
y_softplus = F.softplus(x).data.numpy()
# y_softmax = F.softmax(x)  softmax æ¯”è¾ƒç‰¹æ®Š, ä¸èƒ½ç›´æ¥æ˜¾ç¤º, ä¸è¿‡ä»–æ˜¯å…³äºæ¦‚ç‡çš„, ç”¨äºåˆ†ç±»

import matplotlib.pyplot as plt  # python çš„å¯è§†åŒ–æ¨¡å—, æˆ‘æœ‰æ•™ç¨‹ (https://morvanzhou.github.io/tutorials/data-manipulation/plt/)

plt.figure(1, figsize=(8, 6))
plt.subplot(221)
plt.plot(x_np, y_relu, c='red', label='relu')
plt.ylim((-1, 5))
plt.legend(loc='best')

plt.subplot(222)
plt.plot(x_np, y_sigmoid, c='red', label='sigmoid')
plt.ylim((-0.2, 1.2))
plt.legend(loc='best')

plt.subplot(223)
plt.plot(x_np, y_tanh, c='red', label='tanh')
plt.ylim((-1.2, 1.2))
plt.legend(loc='best')

plt.subplot(224)
plt.plot(x_np, y_softplus, c='red', label='softplus')
plt.ylim((-0.2, 6))
plt.legend(loc='best')

plt.show()
```

![](../.gitbook/assets/image.png)

### â¡ First NN

```python
# Regression Task

import torch
import matplotlib.pyplot as plt

x = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1)  # x data (tensor), shape=(100, 1)
y = x.pow(2) + 0.2*torch.rand(x.size())                 # noisy y data (tensor), shape=(100, 1)

# ç”»å›¾ ,ä¸‹å¥åº”è¯¥åˆ é™¤ data,å¦åˆ™æœ‰é”™
plt.scatter(x.data.numpy(), y.data.numpy())
plt.show()
#======set NN======#
import torch
import torch.nn.functional as F     # æ¿€åŠ±å‡½æ•°éƒ½åœ¨è¿™

class Net(torch.nn.Module):  # ç»§æ‰¿ torch çš„ Module
    def __init__(self, n_feature, n_hidden, n_output):
        super(Net, self).__init__()     # ç»§æ‰¿ __init__ åŠŸèƒ½
        # å®šä¹‰æ¯å±‚ç”¨ä»€ä¹ˆæ ·çš„å½¢å¼
        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # éšè—å±‚çº¿æ€§è¾“å‡º
        self.predict = torch.nn.Linear(n_hidden, n_output)   # è¾“å‡ºå±‚çº¿æ€§è¾“å‡º

    def forward(self, x):   # è¿™åŒæ—¶ä¹Ÿæ˜¯ Module ä¸­çš„ forward åŠŸèƒ½
        # æ­£å‘ä¼ æ’­è¾“å…¥å€¼, ç¥ç»ç½‘ç»œåˆ†æå‡ºè¾“å‡ºå€¼
        x = F.relu(self.hidden(x))      # æ¿€åŠ±å‡½æ•°(éšè—å±‚çš„çº¿æ€§å€¼)
        x = self.predict(x)             # è¾“å‡ºå€¼
        return x

net = Net(n_feature=1, n_hidden=10, n_output=1)

print(net)  # net çš„ç»“æ„
"""
Net (
  (hidden): Linear (1 -> 10)
  (predict): Linear (10 -> 1)
)
"""
# training
# optimizer æ˜¯è®­ç»ƒçš„å·¥å…·
optimizer = torch.optim.SGD(net.parameters(), lr=0.2)  # ä¼ å…¥ net çš„æ‰€æœ‰å‚æ•°, å­¦ä¹ ç‡
loss_func = torch.nn.MSELoss()      # é¢„æµ‹å€¼å’ŒçœŸå®å€¼çš„è¯¯å·®è®¡ç®—å…¬å¼ (å‡æ–¹å·®)

for t in range(100):
    prediction = net(x)     # å–‚ç»™ net è®­ç»ƒæ•°æ® x, è¾“å‡ºé¢„æµ‹å€¼

    loss = loss_func(prediction, y)     # è®¡ç®—ä¸¤è€…çš„è¯¯å·®

    optimizer.zero_grad()   # æ¸…ç©ºä¸Šä¸€æ­¥çš„æ®‹ä½™æ›´æ–°å‚æ•°å€¼
    loss.backward()         # è¯¯å·®åå‘ä¼ æ’­, è®¡ç®—å‚æ•°æ›´æ–°å€¼
    optimizer.step()        # å°†å‚æ•°æ›´æ–°å€¼æ–½åŠ åˆ° net çš„ parameters ä¸Š
    
# visualization
import matplotlib.pyplot as plt

plt.ion()   # ç”»å›¾
plt.show()

for t in range(200):

    ...
    loss.backward()
    optimizer.step()

    # æ¥ç€ä¸Šé¢æ¥
    if t % 5 == 0:
        # plot and show learning process
        plt.cla()
        plt.scatter(x.data.numpy(), y.data.numpy())
        plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5)
        plt.text(0.5, 0, 'Loss=%.4f' % loss.data.numpy(), fontdict={'size': 20, 'color':  'red'})
        plt.pause(0.1)
        
        
___________________________________________________
Classification tasks

```

### â¡ build networks

```python
# first method
class Net(torch.nn.Module):
    def __init__(self, n_feature, n_hidden, n_output):
        super(Net, self).__init__()
        self.hidden = torch.nn.Linear(n_feature, n_hidden)
        self.predict = torch.nn.Linear(n_hidden, n_output)

    def forward(self, x):
        x = F.relu(self.hidden(x))
        x = self.predict(x)
        return x

net1 = Net(1, 10, 1)   # è¿™æ˜¯æˆ‘ä»¬ç”¨è¿™ç§æ–¹å¼æ­å»ºçš„ net1

#second method ğŸŒ€
net2 = torch.nn.Sequential(
    torch.nn.Linear(1, 10),
    torch.nn.ReLU(),
    torch.nn.Linear(10, 1)
)

ç›¸æ¯” net2, net1 çš„å¥½å¤„å°±æ˜¯, ä½ å¯ä»¥æ ¹æ®ä½ çš„ä¸ªäººéœ€è¦æ›´åŠ ä¸ªæ€§åŒ–ä½ è‡ªå·±çš„
å‰å‘ä¼ æ’­è¿‡ç¨‹, æ¯”å¦‚(RNN). ä¸è¿‡å¦‚æœä½ ä¸éœ€è¦ä¸ƒä¸ƒå…«å…«çš„è¿‡ç¨‹, ç›¸ä¿¡ net2 
è¿™ç§å½¢å¼æ›´é€‚åˆä½ .
```

### â¡ save models

```python
#save the trained net1 network
torch.save(net1, 'net.pkl')  # ä¿å­˜æ•´ä¸ªç½‘ç»œ
torch.save(net1.state_dict(), 'net_params.pkl')   
# åªä¿å­˜ç½‘ç»œä¸­çš„å‚æ•° (é€Ÿåº¦å¿«, å å†…å­˜å°‘)

#apply saved model
def restore_net():
    # restore entire net1 to net2
    net2 = torch.load('net.pkl')
    prediction = net2(x)
# apply saved parameters
def restore_params():
    # æ–°å»º net3
    net3 = torch.nn.Sequential(
        torch.nn.Linear(1, 10),
        torch.nn.ReLU(),
        torch.nn.Linear(10, 1)
    )

    # å°†ä¿å­˜çš„å‚æ•°å¤åˆ¶åˆ° net3
    net3.load_state_dict(torch.load('net_params.pkl'))
    prediction = net3(x)
```

### â¡ batch train

DataLoader , åŒ…è£…æ•°æ®,æ‰¹è®­ç»ƒ. æ•°æ®è®­ç»ƒåŒ…æ‹¬è¿™å‡ ç§æ–¹å¼: SGD, Momentum, AdaGrad, RMSProp, Adam

è®­ç»ƒå‰å°† numpy, array ç­‰æ•°æ®å½¢å¼è½¬æ¢æˆ tensor,ç„¶åæ”¾åœ¨è¿™é‡Œ.

```text
import torch
import torch.utils.data as Data
torch.manual_seed(1)    # reproducible

BATCH_SIZE = 5      # æ‰¹è®­ç»ƒçš„æ•°æ®ä¸ªæ•°

x = torch.linspace(1, 10, 10)       # x data (torch tensor)
y = torch.linspace(10, 1, 10)       # y data (torch tensor)

# å…ˆè½¬æ¢æˆ torch èƒ½è¯†åˆ«çš„ Dataset
torch_dataset = Data.TensorDataset(data_tensor=x, target_tensor=y)

# æŠŠ dataset æ”¾å…¥ DataLoader
loader = Data.DataLoader(
    dataset=torch_dataset,      # torch TensorDataset format
    batch_size=BATCH_SIZE,      # mini batch size
    shuffle=True,               # è¦ä¸è¦æ‰“ä¹±æ•°æ® (æ‰“ä¹±æ¯”è¾ƒå¥½)
    num_workers=2,              # å¤šçº¿ç¨‹æ¥è¯»æ•°æ®
)

for epoch in range(3):   # è®­ç»ƒæ‰€æœ‰!æ•´å¥—!æ•°æ® 3 æ¬¡
    for step, (batch_x, batch_y) in enumerate(loader):  # æ¯ä¸€æ­¥ loader é‡Šæ”¾ä¸€å°æ‰¹æ•°æ®ç”¨æ¥å­¦ä¹ 
        # å‡è®¾è¿™é‡Œå°±æ˜¯ä½ è®­ç»ƒçš„åœ°æ–¹...

        # æ‰“å‡ºæ¥ä¸€äº›æ•°æ®
        print('Epoch: ', epoch, '| Step: ', step, '| batch x: ',
              batch_x.numpy(), '| batch y: ', batch_y.numpy())
```

Speed up training  
`SGD` æ˜¯æœ€æ™®é€šçš„ä¼˜åŒ–å™¨, ä¹Ÿå¯ä»¥è¯´æ²¡æœ‰åŠ é€Ÿæ•ˆæœ, è€Œ `Momentum` æ˜¯ `SGD` çš„æ”¹è‰¯ç‰ˆ, å®ƒåŠ å…¥äº†åŠ¨é‡åŸåˆ™. åé¢çš„ `RMSprop` åˆæ˜¯ `Momentum` çš„å‡çº§ç‰ˆ. è€Œ `Adam` åˆæ˜¯ `RMSprop` çš„å‡çº§ç‰ˆ. ä¸è¿‡ä»è¿™ä¸ªç»“æœä¸­æˆ‘ä»¬çœ‹åˆ°, `Adam` çš„æ•ˆæœä¼¼ä¹æ¯” `RMSprop` è¦å·®ä¸€ç‚¹. æ‰€ä»¥è¯´å¹¶ä¸æ˜¯è¶Šå…ˆè¿›çš„ä¼˜åŒ–å™¨, ç»“æœè¶Šä½³. æˆ‘ä»¬åœ¨è‡ªå·±çš„è¯•éªŒä¸­å¯ä»¥å°è¯•ä¸åŒçš„ä¼˜åŒ–å™¨, æ‰¾åˆ°é‚£ä¸ªæœ€é€‚åˆä½ æ•°æ®/ç½‘ç»œçš„ä¼˜åŒ–å™¨.

### â¡ GAN

```python
import torch
import torch.nn as nn
from torch.autograd import Variable
import numpy as np
import matplotlib.pyplot as plt

torch.manual_seed(1)
np.random.seed(1)

BATCH_SIZE = 64
LR_G = 0.0001  #generator
LR_D = 0.0001  #discriminator
N_IDEAS = 5
ART_COMPONENTS = 15
PAINT_POINTS = np.vstack([np.linspace(-1,1,ART_COMPONENTS) for _ in range(BATCH_SIZE)])

def artist_works():
    a = np.random.uniform(1,2,size=BATCH_SIZE)[:,np.newaxis] # random generate an a
    paintings = a * np.power(PAINT_POINTS,2)+(a-1)
    paintings = torch.from_numpy(paintings).float()
    return Variable(paintings)


# define two NNs as follows
G = nn.Sequential(
        nn.Linear(N_IDEAS,128),
        nn.ReLU(),
        nn.Linear(128,ART_COMPONENTS)
        
        )

D = nn.Sequential(
        nn.Linear(ART_COMPONENTS,128),
        nn.ReLU(),
        nn.Linear(128,1),
        nn.Sigmoid(),
        ) 
    
opt_D = torch.optim.Adam(D.parameters(),lr=LR_D)
opt_G = torch.optim.Adam(G.parameters(),lr=LR_G)

# ä¸‹é¢è¿™ä¸ªè¿‡ç¨‹:æè¿°:
# aritst_paintings æ˜¯ç”¨ artist_works å‡½æ•°ç”Ÿæˆçš„æ ‡å‡†ç­”æ¡ˆ
# G_paintings æ˜¯ G ç”Ÿæˆçš„ç­”æ¡ˆ
# D åˆ†åˆ«å¯¹äºŒè€…ä½œä¸ºè¾“å…¥å½¢æˆä¸åŒçš„è¾“å‡º prob_artist0,1
# D_loss ä¸ºäº†
# G_loss ä¸ºäº†è®©
for step in range(20001):
    artist_paintings = artist_works()   #è‘—åç”»å®¶çš„ç”»
    G_ideas = Variable(torch.randn(BATCH_SIZE,N_IDEAS))
    G_paintings = G(G_ideas)   #æ–°æ‰‹ç”»å®¶çš„ç”».ç”¨ ideas ç”Ÿæˆçš„.
    
    prob_artist0 = D(artist_paintings)
    prob_artist1 = D(G_paintings)
    
    D_loss = - torch.mean(torch.log(prob_artist0)+torch.log(1-prob_artist1))
    G_loss = torch.mean(torch.log(1-prob_artist1))    
    
    opt_D.zero_grad()
    D_loss.backward(retain_graph=True)
    opt_D.step()

    opt_G.zero_grad()
    G_loss.backward()
    opt_G.step()    
    
    
    if step % 50 == 0:  # plotting
        plt.cla()
        plt.plot(PAINT_POINTS[0], G_paintings.data.numpy()[0], c='#4AD631', lw=3, label='Generated painting',)
        plt.plot(PAINT_POINTS[0], 2 * np.power(PAINT_POINTS[0], 2) + 1, c='#74BCFF', lw=3, label='upper bound')
        plt.plot(PAINT_POINTS[0], 1 * np.power(PAINT_POINTS[0], 2) + 0, c='#FF9359', lw=3, label='lower bound')
        plt.text(-.5, 2.3, 'D accuracy=%.2f (0.5 for D to converge)' % prob_artist0.data.numpy().mean(), fontdict={'size': 13})
        plt.text(-.5, 2, 'D score= %.2f (-1.38 for G to converge)' % -D_loss.data.numpy(), fontdict={'size': 13})
        plt.text(-.5,1.5,'setp: %d /20000'% step)
        plt.ylim((0, 3));plt.legend(loc='upper right', fontsize=10);plt.draw();plt.pause(0.01)

plt.ioff()
plt.show()
```

### â¡ Torch's dynamic state

åŠ¨æ€çš„è®¡ç®—å›¾,ä¸ä¹‹å¯¹åº”çš„äº‹ TensorFlowçš„é™æ€å›¾.



## å…¶ä»–



æ˜¾ç¤ºç‰ˆæœ¬:  
import torch   
print\(torch.\_\_version\_\_\)

PyTorch ä¼šå®‰è£…ä¸¤ä¸ªæ¨¡å—, ä¸€ä¸ªæ˜¯ torch, ä¸€ä¸ª torchvision, torch æ˜¯ä¸»æ¨¡å—, ç”¨æ¥æ­å»ºç¥ç»ç½‘ç»œçš„, torchvision æ˜¯è¾…æ¨¡å—, æœ‰æ•°æ®åº“, è¿˜æœ‰ä¸€äº›å·²ç»è®­ç»ƒå¥½çš„ç¥ç»ç½‘ç»œç­‰ç€ä½ ç›´æ¥ç”¨, æ¯”å¦‚ \([VGG, AlexNet, ResNet](http://pytorch.org/docs/torchvision/models.html)\).



## Nan

## Nan

